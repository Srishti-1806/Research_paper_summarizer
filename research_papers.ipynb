{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srishti-1806/Research_paper_summarizer/blob/main/research_papers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGdHcsE6vS-D",
        "outputId": "f71b37e8-2be8-4b78-f59d-08764af0e964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=fe4a4ebf63362a16f1b3183897dc03a14659fbc61d2e718be72e020bb65697bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFGD5oive0O",
        "outputId": "ece31694-322f-4d05-aaa2-eaca12fb0bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "\n",
        "#query to fetch propellerless drones\n",
        "query = \"Detecting Drones By Finding Propellers For Mid-Air Landing And Following\"\n",
        "# Reduced max_results to a more conservative number\n",
        "search = arxiv.Search(query = query, max_results = 200, sort_by = arxiv.SortCriterion.Relevance)\n",
        "\n",
        "#fetch papers\n",
        "papers=[]\n",
        "for result in search.results():\n",
        "  papers.append({\n",
        "      \"published\": result.published,\n",
        "      \"title\": result.title,\n",
        "      \"summary\": result.summary,\n",
        "      \"url\": result.pdf_url\n",
        "  })\n",
        "\n",
        "# The DataFrame creation and display should be outside the loop\n",
        "# otherwise it will create and display the dataframe for each paper fetched\n",
        "df= pd.DataFrame(papers)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5xHzloyJvmSF",
        "outputId": "76ea70ae-8523-4ccc-8375-c532b2794166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-2cacdbbf86c6>:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2021-06-29 01:16:01+00:00   \n",
              "1 2024-07-26 21:05:04+00:00   \n",
              "2 2022-06-09 17:09:16+00:00   \n",
              "3 2022-02-15 04:04:24+00:00   \n",
              "4 2020-03-11 06:13:44+00:00   \n",
              "5 2024-03-11 10:20:44+00:00   \n",
              "6 2023-02-01 22:41:46+00:00   \n",
              "7 2021-06-16 15:51:42+00:00   \n",
              "8 2023-08-19 03:57:52+00:00   \n",
              "9 2025-03-02 08:40:23+00:00   \n",
              "\n",
              "                                                                                                                    title  \\\n",
              "0                                     EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following   \n",
              "1                                                                   Propeller Modulation Equalization via Reference Tones   \n",
              "2                          Autonomous Drone Landing with Fiducial Markers and a Gimbal-Mounted Camera for Active Tracking   \n",
              "3                     Tombo Propeller: Bio-Inspired Deformable Structure toward Collision-Accommodated Control for Drones   \n",
              "4                                                                                         Hundred Drones Land in a Minute   \n",
              "5                                Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings   \n",
              "6                                                 Autonomous Drone Landing: Marked Landing Pads and Solidified Lava Flows   \n",
              "7                                                                       Autonomous Navigation System for a Delivery Drone   \n",
              "8  Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models   \n",
              "9            Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                summary  \\\n",
              "0                                                                                         The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\\nthreat to general security and confidentiality. Most of the commercially\\navailable or custom-built drones are multi-rotors and are comprised of multiple\\npropellers. Since these propellers rotate at a high-speed, they are generally\\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\\nclassical camera without severe motion blur. We utilize a class of sensors that\\nare particularly suitable for such scenarios called event cameras, which have a\\nhigh temporal resolution, low-latency, and high dynamic range.\\n  In this paper, we model the geometry of a propeller and use it to generate\\nsimulated events which are used to train a deep neural network called EVPropNet\\nto detect propellers from the data of an event camera. EVPropNet directly\\ntransfers to the real world without any fine-tuning or retraining. We present\\ntwo applications of our network: (a) tracking and following an unmarked drone\\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\\nthe proposed approach in many real-world experiments with different propeller\\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\\nbudget. To our knowledge, this is the first deep learning-based solution for\\ndetecting propellers (to detect drones). Finally, our applications also show an\\nimpressive success rate of 92% and 90% for the tracking and landing tasks\\nrespectively.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Propeller modulation, also known as micro-Doppler modulation, presents a\\nsignificant challenge in radio frequency (RF) inspection operations conducted\\nvia drones. This paper investigates the equalization of propeller modulation\\neffects on RF signals, specifically targeting applications in navigation aids\\nsuch as Instrument Landing Systems (ILS). By employing a continuous reference\\ntone, the propeller-induced Doppler spread can be effectively captured and\\nequalized, improving signal integrity and accuracy. Simulation results\\ndemonstrate that the proposed equalization method significantly reduces DDM\\ndeviation caused by propeller modulation, even under various propeller speeds.\\nThe findings suggest that incorporating such equalization techniques can\\nenhance the reliability and efficiency of drone-based RF inspections.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Precision landing is a remaining challenge in autonomous drone flight.\\nFiducial markers provide a computationally cheap way for a drone to locate a\\nlanding pad and autonomously execute precision landings. However, most work in\\nthis field depends on either rigidly-mounted or downward-facing cameras which\\nrestrict the drone's ability to detect the marker. We present a method of\\nautonomous landing that uses a gimbal-mounted camera to quickly search for the\\nlanding pad by simply spinning in place while tilting the camera up and down,\\nand to continually aim the camera at the landing pad during approach and\\nlanding. This method demonstrates successful search, tracking, and landing with\\n4 of 5 tested fiducial systems on a physical drone with no human intervention.\\nPer fiducial system, we present the distributions of the distances from the\\ndrone to the center of the landing pad after each successful landing. We also\\nshow representative examples of flight trajectories, marker tracking\\nperformance, and control outputs for each channel during the landing. Finally,\\nwe discuss qualitative strengths and weaknesses underlying each system.   \n",
              "3  There is a growing need for vertical take-off and landing vehicles, including\\ndrones, which are safe to use and can adapt to collisions. The risks of damage\\nby collision, to humans, obstacles in the environment, and drones themselves,\\nare significant. This has prompted a search into nature for a highly resilient\\nstructure that can inform a design of propellers to reduce those risks and\\nenhance safety. Inspired by the flexibility and resilience of dragonfly wings,\\nwe propose a novel design for a biomimetic drone propeller called Tombo\\npropeller. Here, we report on the design and fabrication process of this\\nbiomimetic propeller that can accommodate collisions and recover quickly, while\\nmaintaining sufficient thrust force to hover and fly. We describe the\\ndevelopment of an aerodynamic model and experiments conducted to investigate\\nperformance characteristics for various configurations of the propeller\\nmorphology, and related properties, such as generated thrust force, thrust\\nforce deviation, collision force, recovery time, lift-to-drag ratio, and noise.\\nFinally, we design and showcase a control strategy for a drone equipped with\\nTombo propellers that collides in mid-air with an obstacle and recovers from\\ncollision continuing flying. The results show that the maximum collision force\\ngenerated by the proposed Tombo propeller is less than two-thirds that of a\\ntraditional rigid propeller, which suggests the concrete possibility to employ\\ndeformable propellers for drones flying in a cluttered environment. This\\nresearch can contribute to morphological design of flying vehicles for agile\\nand resilient performance.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                         Currently, drone research and development has received significant attention\\nworldwide. Particularly, delivery services employ drones as it is a viable\\nmethod to improve delivery efficiency by using a several unmanned drones.\\nResearch has been conducted to realize complete automation of drone control for\\nsuch services. However, regarding the takeoff and landing port of the drones,\\nconventional methods have focused on the landing operation of a single drone,\\nand the continuous landing of multiple drones has not been realized. To address\\nthis issue, we propose a completely novel port system, \"EAGLES Port,\" that\\nallows several drones to continuously land and takeoff in a short time.\\nExperiments verified that the landing time efficiency of the proposed port is\\nideally 7.5 times higher than that of conventional vertical landing systems.\\nMoreover, the system can tolerate 270 mm of horizontal positional error, +-30\\ndeg of angular error in the drone's approach (+-40 deg with the proposed gate\\nmechanism), and up to 1.9 m/s of drone's approach speed. This technology\\nsignificantly contributes to the scalability of drone usage. Therefore, it is\\ncritical for the development of a future drone port for the landing of\\nautomated drone swarms.   \n",
              "5                                                                                                         Mastering autonomous drone landing on dynamic platforms presents formidable\\nchallenges due to unpredictable velocities and external disturbances caused by\\nthe wind, ground effect, turbines or propellers of the docking platform. This\\nstudy introduces an advanced Deep Reinforcement Learning (DRL) agent,\\nLander:AI, designed to navigate and land on platforms in the presence of windy\\nconditions, thereby enhancing drone autonomy and safety. Lander:AI is\\nrigorously trained within the gym-pybullet-drone simulation, an environment\\nthat mirrors real-world complexities, including wind turbulence, to ensure the\\nagent's robustness and adaptability.\\n  The agent's capabilities were empirically validated with Crazyflie 2.1 drones\\nacross various test scenarios, encompassing both simulated environments and\\nreal-world conditions. The experimental results showcased Lander:AI's\\nhigh-precision landing and its ability to adapt to moving platforms, even under\\nwind-induced disturbances. Furthermore, the system performance was benchmarked\\nagainst a baseline PID controller augmented with an Extended Kalman Filter,\\nillustrating significant improvements in landing precision and error recovery.\\nLander:AI leverages bio-inspired learning to adapt to external forces like\\nbirds, enhancing drone adaptability without knowing force magnitudes.This\\nresearch not only advances drone landing technologies, essential for inspection\\nand emergency applications, but also highlights the potential of DRL in\\naddressing intricate aerodynamic challenges.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                    Landing is the most challenging and risky aspect of multirotor drone flight,\\nand only simple landing methods exist for autonomous drones. We explore methods\\nfor autonomous drone landing in two scenarios. In the first scenario, we\\nexamine methods for landing on known landing pads using fiducial markers and a\\ngimbal-mounted monocular camera. This method has potential in drone\\napplications where a drone must land more accurately than GPS can provide\\n(e.g.~package delivery in an urban canyon). We expand on previous methods by\\nactuating the drone's camera to track the marker over time, and we address the\\ncomplexities of pose estimation caused by fiducial marker orientation\\nambiguity. In the second scenario, and in collaboration with the RAVEN project,\\nwe explore methods for landing on solidified lava flows in Iceland, which\\nserves as an analog environment for Mars and provides insight into the\\neffectiveness of drone-rover exploration teams. Our drone uses a depth camera\\nto visualize the terrain, and we are developing methods to analyze the terrain\\ndata for viable landing sites in real time with minimal sensors and external\\ninfrastructure requirements, so that the solution does not heavily influence\\nthe drone's behavior, mission structure, or operational environments.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The use of delivery services is an increasing trend worldwide, further\\nenhanced by the COVID pandemic. In this context, drone delivery systems are of\\ngreat interest as they may allow for faster and cheaper deliveries. This paper\\npresents a navigation system that makes feasible the delivery of parcels with\\nautonomous drones. The system generates a path between a start and a final\\npoint and controls the drone to follow this path based on its localization\\nobtained through GPS, 9DoF IMU, and barometer. In the landing phase,\\ninformation of poses estimated by a marker (ArUco) detection technique using a\\ncamera, ultra-wideband (UWB) devices, and the drone's software estimation are\\nmerged by utilizing an Extended Kalman Filter algorithm to improve the landing\\nprecision. A vector field-based method controls the drone to follow the desired\\npath smoothly, reducing vibrations or harsh movements that could harm the\\ntransported parcel. Real experiments validate the delivery strategy and allow\\nto evaluate the performance of the adopted techniques. Preliminary results\\nstate the viability of our proposal for autonomous drone delivery.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Accurate drone detection is strongly desired in drone collision avoidance,\\ndrone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With\\nthe recent emergence of the Vision Transformer (ViT), this critical task is\\nreassessed in this paper using a UAV dataset composed of 1359 drone photos. We\\nconstruct various CNN and ViT-based models, demonstrating that for single-drone\\ndetection, a basic ViT can achieve performance 4.6 times more robust than our\\nbest CNN-based transfer learning models. By implementing the state-of-the-art\\nYou Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You\\nOnly Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we\\nattain impressive 98% and 96% mAP values, respectively. We find that ViT\\noutperforms CNN at the same epoch, but also requires more training data,\\ncomputational power, and sophisticated, performance-oriented designs to fully\\nsurpass the capabilities of cutting-edge CNN detectors. We summarize the\\ndistinct characteristics of ViT and CNN models to aid future researchers in\\ndeveloping more efficient deep learning models.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The imminent commercialization of UAM requires stable, AI-based maintenance\\nsystems to ensure safety for both passengers and pedestrians. This paper\\npresents a methodology for non-destructively detecting cracks in UAM propellers\\nusing drone propeller sound datasets. Normal operating sounds were recorded,\\nand abnormal sounds (categorized as ripped and broken) were differentiated by\\nvarying the microphone-propeller angle and throttle power. Our novel approach\\nintegrates FFT and STFT preprocessing techniques to capture both global\\nfrequency patterns and local time-frequency variations, thereby enhancing\\nanomaly detection performance. The constructed Acoustic Dataset for Crack of\\nDrone Propeller (ADCP) demonstrates the potential for detecting propeller\\ncracks and lays the groundwork for future UAM maintenance applications.   \n",
              "\n",
              "                                 url  \n",
              "0  http://arxiv.org/pdf/2106.15045v1  \n",
              "1  http://arxiv.org/pdf/2407.19084v1  \n",
              "2  http://arxiv.org/pdf/2206.04617v3  \n",
              "3  http://arxiv.org/pdf/2202.07177v1  \n",
              "4  http://arxiv.org/pdf/2003.05127v1  \n",
              "5  http://arxiv.org/pdf/2403.06572v2  \n",
              "6  http://arxiv.org/pdf/2302.00786v1  \n",
              "7  http://arxiv.org/pdf/2106.08878v1  \n",
              "8  http://arxiv.org/pdf/2308.09899v2  \n",
              "9  http://arxiv.org/pdf/2503.00790v1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3fd56548-a24b-4fff-b5fd-34bd84cdd0b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-06-29 01:16:01+00:00</td>\n",
              "      <td>EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following</td>\n",
              "      <td>The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\\nthreat to general security and confidentiality. Most of the commercially\\navailable or custom-built drones are multi-rotors and are comprised of multiple\\npropellers. Since these propellers rotate at a high-speed, they are generally\\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\\nclassical camera without severe motion blur. We utilize a class of sensors that\\nare particularly suitable for such scenarios called event cameras, which have a\\nhigh temporal resolution, low-latency, and high dynamic range.\\n  In this paper, we model the geometry of a propeller and use it to generate\\nsimulated events which are used to train a deep neural network called EVPropNet\\nto detect propellers from the data of an event camera. EVPropNet directly\\ntransfers to the real world without any fine-tuning or retraining. We present\\ntwo applications of our network: (a) tracking and following an unmarked drone\\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\\nthe proposed approach in many real-world experiments with different propeller\\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\\nbudget. To our knowledge, this is the first deep learning-based solution for\\ndetecting propellers (to detect drones). Finally, our applications also show an\\nimpressive success rate of 92% and 90% for the tracking and landing tasks\\nrespectively.</td>\n",
              "      <td>http://arxiv.org/pdf/2106.15045v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-07-26 21:05:04+00:00</td>\n",
              "      <td>Propeller Modulation Equalization via Reference Tones</td>\n",
              "      <td>Propeller modulation, also known as micro-Doppler modulation, presents a\\nsignificant challenge in radio frequency (RF) inspection operations conducted\\nvia drones. This paper investigates the equalization of propeller modulation\\neffects on RF signals, specifically targeting applications in navigation aids\\nsuch as Instrument Landing Systems (ILS). By employing a continuous reference\\ntone, the propeller-induced Doppler spread can be effectively captured and\\nequalized, improving signal integrity and accuracy. Simulation results\\ndemonstrate that the proposed equalization method significantly reduces DDM\\ndeviation caused by propeller modulation, even under various propeller speeds.\\nThe findings suggest that incorporating such equalization techniques can\\nenhance the reliability and efficiency of drone-based RF inspections.</td>\n",
              "      <td>http://arxiv.org/pdf/2407.19084v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-06-09 17:09:16+00:00</td>\n",
              "      <td>Autonomous Drone Landing with Fiducial Markers and a Gimbal-Mounted Camera for Active Tracking</td>\n",
              "      <td>Precision landing is a remaining challenge in autonomous drone flight.\\nFiducial markers provide a computationally cheap way for a drone to locate a\\nlanding pad and autonomously execute precision landings. However, most work in\\nthis field depends on either rigidly-mounted or downward-facing cameras which\\nrestrict the drone's ability to detect the marker. We present a method of\\nautonomous landing that uses a gimbal-mounted camera to quickly search for the\\nlanding pad by simply spinning in place while tilting the camera up and down,\\nand to continually aim the camera at the landing pad during approach and\\nlanding. This method demonstrates successful search, tracking, and landing with\\n4 of 5 tested fiducial systems on a physical drone with no human intervention.\\nPer fiducial system, we present the distributions of the distances from the\\ndrone to the center of the landing pad after each successful landing. We also\\nshow representative examples of flight trajectories, marker tracking\\nperformance, and control outputs for each channel during the landing. Finally,\\nwe discuss qualitative strengths and weaknesses underlying each system.</td>\n",
              "      <td>http://arxiv.org/pdf/2206.04617v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-02-15 04:04:24+00:00</td>\n",
              "      <td>Tombo Propeller: Bio-Inspired Deformable Structure toward Collision-Accommodated Control for Drones</td>\n",
              "      <td>There is a growing need for vertical take-off and landing vehicles, including\\ndrones, which are safe to use and can adapt to collisions. The risks of damage\\nby collision, to humans, obstacles in the environment, and drones themselves,\\nare significant. This has prompted a search into nature for a highly resilient\\nstructure that can inform a design of propellers to reduce those risks and\\nenhance safety. Inspired by the flexibility and resilience of dragonfly wings,\\nwe propose a novel design for a biomimetic drone propeller called Tombo\\npropeller. Here, we report on the design and fabrication process of this\\nbiomimetic propeller that can accommodate collisions and recover quickly, while\\nmaintaining sufficient thrust force to hover and fly. We describe the\\ndevelopment of an aerodynamic model and experiments conducted to investigate\\nperformance characteristics for various configurations of the propeller\\nmorphology, and related properties, such as generated thrust force, thrust\\nforce deviation, collision force, recovery time, lift-to-drag ratio, and noise.\\nFinally, we design and showcase a control strategy for a drone equipped with\\nTombo propellers that collides in mid-air with an obstacle and recovers from\\ncollision continuing flying. The results show that the maximum collision force\\ngenerated by the proposed Tombo propeller is less than two-thirds that of a\\ntraditional rigid propeller, which suggests the concrete possibility to employ\\ndeformable propellers for drones flying in a cluttered environment. This\\nresearch can contribute to morphological design of flying vehicles for agile\\nand resilient performance.</td>\n",
              "      <td>http://arxiv.org/pdf/2202.07177v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-11 06:13:44+00:00</td>\n",
              "      <td>Hundred Drones Land in a Minute</td>\n",
              "      <td>Currently, drone research and development has received significant attention\\nworldwide. Particularly, delivery services employ drones as it is a viable\\nmethod to improve delivery efficiency by using a several unmanned drones.\\nResearch has been conducted to realize complete automation of drone control for\\nsuch services. However, regarding the takeoff and landing port of the drones,\\nconventional methods have focused on the landing operation of a single drone,\\nand the continuous landing of multiple drones has not been realized. To address\\nthis issue, we propose a completely novel port system, \"EAGLES Port,\" that\\nallows several drones to continuously land and takeoff in a short time.\\nExperiments verified that the landing time efficiency of the proposed port is\\nideally 7.5 times higher than that of conventional vertical landing systems.\\nMoreover, the system can tolerate 270 mm of horizontal positional error, +-30\\ndeg of angular error in the drone's approach (+-40 deg with the proposed gate\\nmechanism), and up to 1.9 m/s of drone's approach speed. This technology\\nsignificantly contributes to the scalability of drone usage. Therefore, it is\\ncritical for the development of a future drone port for the landing of\\nautomated drone swarms.</td>\n",
              "      <td>http://arxiv.org/pdf/2003.05127v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-03-11 10:20:44+00:00</td>\n",
              "      <td>Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings</td>\n",
              "      <td>Mastering autonomous drone landing on dynamic platforms presents formidable\\nchallenges due to unpredictable velocities and external disturbances caused by\\nthe wind, ground effect, turbines or propellers of the docking platform. This\\nstudy introduces an advanced Deep Reinforcement Learning (DRL) agent,\\nLander:AI, designed to navigate and land on platforms in the presence of windy\\nconditions, thereby enhancing drone autonomy and safety. Lander:AI is\\nrigorously trained within the gym-pybullet-drone simulation, an environment\\nthat mirrors real-world complexities, including wind turbulence, to ensure the\\nagent's robustness and adaptability.\\n  The agent's capabilities were empirically validated with Crazyflie 2.1 drones\\nacross various test scenarios, encompassing both simulated environments and\\nreal-world conditions. The experimental results showcased Lander:AI's\\nhigh-precision landing and its ability to adapt to moving platforms, even under\\nwind-induced disturbances. Furthermore, the system performance was benchmarked\\nagainst a baseline PID controller augmented with an Extended Kalman Filter,\\nillustrating significant improvements in landing precision and error recovery.\\nLander:AI leverages bio-inspired learning to adapt to external forces like\\nbirds, enhancing drone adaptability without knowing force magnitudes.This\\nresearch not only advances drone landing technologies, essential for inspection\\nand emergency applications, but also highlights the potential of DRL in\\naddressing intricate aerodynamic challenges.</td>\n",
              "      <td>http://arxiv.org/pdf/2403.06572v2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-02-01 22:41:46+00:00</td>\n",
              "      <td>Autonomous Drone Landing: Marked Landing Pads and Solidified Lava Flows</td>\n",
              "      <td>Landing is the most challenging and risky aspect of multirotor drone flight,\\nand only simple landing methods exist for autonomous drones. We explore methods\\nfor autonomous drone landing in two scenarios. In the first scenario, we\\nexamine methods for landing on known landing pads using fiducial markers and a\\ngimbal-mounted monocular camera. This method has potential in drone\\napplications where a drone must land more accurately than GPS can provide\\n(e.g.~package delivery in an urban canyon). We expand on previous methods by\\nactuating the drone's camera to track the marker over time, and we address the\\ncomplexities of pose estimation caused by fiducial marker orientation\\nambiguity. In the second scenario, and in collaboration with the RAVEN project,\\nwe explore methods for landing on solidified lava flows in Iceland, which\\nserves as an analog environment for Mars and provides insight into the\\neffectiveness of drone-rover exploration teams. Our drone uses a depth camera\\nto visualize the terrain, and we are developing methods to analyze the terrain\\ndata for viable landing sites in real time with minimal sensors and external\\ninfrastructure requirements, so that the solution does not heavily influence\\nthe drone's behavior, mission structure, or operational environments.</td>\n",
              "      <td>http://arxiv.org/pdf/2302.00786v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-06-16 15:51:42+00:00</td>\n",
              "      <td>Autonomous Navigation System for a Delivery Drone</td>\n",
              "      <td>The use of delivery services is an increasing trend worldwide, further\\nenhanced by the COVID pandemic. In this context, drone delivery systems are of\\ngreat interest as they may allow for faster and cheaper deliveries. This paper\\npresents a navigation system that makes feasible the delivery of parcels with\\nautonomous drones. The system generates a path between a start and a final\\npoint and controls the drone to follow this path based on its localization\\nobtained through GPS, 9DoF IMU, and barometer. In the landing phase,\\ninformation of poses estimated by a marker (ArUco) detection technique using a\\ncamera, ultra-wideband (UWB) devices, and the drone's software estimation are\\nmerged by utilizing an Extended Kalman Filter algorithm to improve the landing\\nprecision. A vector field-based method controls the drone to follow the desired\\npath smoothly, reducing vibrations or harsh movements that could harm the\\ntransported parcel. Real experiments validate the delivery strategy and allow\\nto evaluate the performance of the adopted techniques. Preliminary results\\nstate the viability of our proposal for autonomous drone delivery.</td>\n",
              "      <td>http://arxiv.org/pdf/2106.08878v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-08-19 03:57:52+00:00</td>\n",
              "      <td>Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models</td>\n",
              "      <td>Accurate drone detection is strongly desired in drone collision avoidance,\\ndrone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With\\nthe recent emergence of the Vision Transformer (ViT), this critical task is\\nreassessed in this paper using a UAV dataset composed of 1359 drone photos. We\\nconstruct various CNN and ViT-based models, demonstrating that for single-drone\\ndetection, a basic ViT can achieve performance 4.6 times more robust than our\\nbest CNN-based transfer learning models. By implementing the state-of-the-art\\nYou Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You\\nOnly Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we\\nattain impressive 98% and 96% mAP values, respectively. We find that ViT\\noutperforms CNN at the same epoch, but also requires more training data,\\ncomputational power, and sophisticated, performance-oriented designs to fully\\nsurpass the capabilities of cutting-edge CNN detectors. We summarize the\\ndistinct characteristics of ViT and CNN models to aid future researchers in\\ndeveloping more efficient deep learning models.</td>\n",
              "      <td>http://arxiv.org/pdf/2308.09899v2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-03-02 08:40:23+00:00</td>\n",
              "      <td>Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)</td>\n",
              "      <td>The imminent commercialization of UAM requires stable, AI-based maintenance\\nsystems to ensure safety for both passengers and pedestrians. This paper\\npresents a methodology for non-destructively detecting cracks in UAM propellers\\nusing drone propeller sound datasets. Normal operating sounds were recorded,\\nand abnormal sounds (categorized as ripped and broken) were differentiated by\\nvarying the microphone-propeller angle and throttle power. Our novel approach\\nintegrates FFT and STFT preprocessing techniques to capture both global\\nfrequency patterns and local time-frequency variations, thereby enhancing\\nanomaly detection performance. The constructed Acoustic Dataset for Crack of\\nDrone Propeller (ADCP) demonstrates the potential for detecting propeller\\ncracks and lays the groundwork for future UAM maintenance applications.</td>\n",
              "      <td>http://arxiv.org/pdf/2503.00790v1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fd56548-a24b-4fff-b5fd-34bd84cdd0b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3fd56548-a24b-4fff-b5fd-34bd84cdd0b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3fd56548-a24b-4fff-b5fd-34bd84cdd0b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-268a30b7-4452-4f9f-8604-24c127232b29\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-268a30b7-4452-4f9f-8604-24c127232b29')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-268a30b7-4452-4f9f-8604-24c127232b29 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-03-11 06:13:44+00:00\",\n        \"max\": \"2025-03-02 08:40:23+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2023-08-19 03:57:52+00:00\",\n          \"2024-07-26 21:05:04+00:00\",\n          \"2024-03-11 10:20:44+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models\",\n          \"Propeller Modulation Equalization via Reference Tones\",\n          \"Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Accurate drone detection is strongly desired in drone collision avoidance,\\ndrone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With\\nthe recent emergence of the Vision Transformer (ViT), this critical task is\\nreassessed in this paper using a UAV dataset composed of 1359 drone photos. We\\nconstruct various CNN and ViT-based models, demonstrating that for single-drone\\ndetection, a basic ViT can achieve performance 4.6 times more robust than our\\nbest CNN-based transfer learning models. By implementing the state-of-the-art\\nYou Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You\\nOnly Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we\\nattain impressive 98% and 96% mAP values, respectively. We find that ViT\\noutperforms CNN at the same epoch, but also requires more training data,\\ncomputational power, and sophisticated, performance-oriented designs to fully\\nsurpass the capabilities of cutting-edge CNN detectors. We summarize the\\ndistinct characteristics of ViT and CNN models to aid future researchers in\\ndeveloping more efficient deep learning models.\",\n          \"Propeller modulation, also known as micro-Doppler modulation, presents a\\nsignificant challenge in radio frequency (RF) inspection operations conducted\\nvia drones. This paper investigates the equalization of propeller modulation\\neffects on RF signals, specifically targeting applications in navigation aids\\nsuch as Instrument Landing Systems (ILS). By employing a continuous reference\\ntone, the propeller-induced Doppler spread can be effectively captured and\\nequalized, improving signal integrity and accuracy. Simulation results\\ndemonstrate that the proposed equalization method significantly reduces DDM\\ndeviation caused by propeller modulation, even under various propeller speeds.\\nThe findings suggest that incorporating such equalization techniques can\\nenhance the reliability and efficiency of drone-based RF inspections.\",\n          \"Mastering autonomous drone landing on dynamic platforms presents formidable\\nchallenges due to unpredictable velocities and external disturbances caused by\\nthe wind, ground effect, turbines or propellers of the docking platform. This\\nstudy introduces an advanced Deep Reinforcement Learning (DRL) agent,\\nLander:AI, designed to navigate and land on platforms in the presence of windy\\nconditions, thereby enhancing drone autonomy and safety. Lander:AI is\\nrigorously trained within the gym-pybullet-drone simulation, an environment\\nthat mirrors real-world complexities, including wind turbulence, to ensure the\\nagent's robustness and adaptability.\\n  The agent's capabilities were empirically validated with Crazyflie 2.1 drones\\nacross various test scenarios, encompassing both simulated environments and\\nreal-world conditions. The experimental results showcased Lander:AI's\\nhigh-precision landing and its ability to adapt to moving platforms, even under\\nwind-induced disturbances. Furthermore, the system performance was benchmarked\\nagainst a baseline PID controller augmented with an Extended Kalman Filter,\\nillustrating significant improvements in landing precision and error recovery.\\nLander:AI leverages bio-inspired learning to adapt to external forces like\\nbirds, enhancing drone adaptability without knowing force magnitudes.This\\nresearch not only advances drone landing technologies, essential for inspection\\nand emergency applications, but also highlights the potential of DRL in\\naddressing intricate aerodynamic challenges.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2308.09899v2\",\n          \"http://arxiv.org/pdf/2407.19084v1\",\n          \"http://arxiv.org/pdf/2403.06572v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_snbCsV1KxG",
        "outputId": "61bc716f-dae8-4b2d-abee-da7966fbff22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed transformers-4.52.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# Ensure transformers is installed\n",
        "!pip install transformers\n",
        "# %%\n",
        "# Ensure arxiv is installed\n",
        "!pip install arxiv\n",
        "# %%\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "\n",
        "#query to fetch propellerless drones\n",
        "query = \"Detecting Drones By Finding Propellers For Mid-Air Landing And Following\"\n",
        "# Reduced max_results to a more conservative number\n",
        "search = arxiv.Search(query = query, max_results = 200, sort_by = arxiv.SortCriterion.Relevance)\n",
        "\n",
        "#fetch papers\n",
        "papers=[]\n",
        "for result in search.results():\n",
        "  papers.append({\n",
        "      \"published\": result.published,\n",
        "      \"title\": result.title,\n",
        "      \"summary\": result.summary,\n",
        "      \"url\": result.pdf_url\n",
        "  })\n",
        "\n",
        "# The DataFrame creation and display should be outside the loop\n",
        "# otherwise it will create and display the dataframe for each paper fetched\n",
        "df= pd.DataFrame(papers)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df.head(10))\n",
        "# %%\n",
        "# Ensure transformers is updated\n",
        "!python -m pip install -U transformers\n",
        "# %%\n",
        "# Uninstall the existing transformers library to ensure a clean installation\n",
        "!pip uninstall -y transformers\n",
        "\n",
        "# Install the transformers library again, ensuring the latest version is installed\n",
        "!pip install -U transformers\n",
        "\n",
        "# Clear the pip cache to remove any potentially corrupted cached data\n",
        "!pip cache purge\n",
        "\n",
        "# Now, import the pipeline function\n",
        "from transformers import pipeline\n",
        "\n",
        "# The rest of your code to use the pipeline\n",
        "abstract = df['summary'][3]\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "summarization_result = summarizer(abstract)\n",
        "# The summarization_result is a list containing dictionaries, so you need to access the element at index 0\n",
        "# then access the 'summary_text' key. The original code had an incorrect index [3].\n",
        "print(summarization_result[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DXjK-C3H1dkm",
        "outputId": "3860bd4c-79a3-465a-cff6-1a4a178c5cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.11/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-b7c6e621d7ec>:20: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(summarization_result[0]['summary_text'])\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-03-11 06:13:44+00:00\",\n        \"max\": \"2025-03-02 08:40:23+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2023-08-19 03:57:52+00:00\",\n          \"2024-07-26 21:05:04+00:00\",\n          \"2024-03-11 10:20:44+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models\",\n          \"Propeller Modulation Equalization via Reference Tones\",\n          \"Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Accurate drone detection is strongly desired in drone collision avoidance,\\ndrone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With\\nthe recent emergence of the Vision Transformer (ViT), this critical task is\\nreassessed in this paper using a UAV dataset composed of 1359 drone photos. We\\nconstruct various CNN and ViT-based models, demonstrating that for single-drone\\ndetection, a basic ViT can achieve performance 4.6 times more robust than our\\nbest CNN-based transfer learning models. By implementing the state-of-the-art\\nYou Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You\\nOnly Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we\\nattain impressive 98% and 96% mAP values, respectively. We find that ViT\\noutperforms CNN at the same epoch, but also requires more training data,\\ncomputational power, and sophisticated, performance-oriented designs to fully\\nsurpass the capabilities of cutting-edge CNN detectors. We summarize the\\ndistinct characteristics of ViT and CNN models to aid future researchers in\\ndeveloping more efficient deep learning models.\",\n          \"Propeller modulation, also known as micro-Doppler modulation, presents a\\nsignificant challenge in radio frequency (RF) inspection operations conducted\\nvia drones. This paper investigates the equalization of propeller modulation\\neffects on RF signals, specifically targeting applications in navigation aids\\nsuch as Instrument Landing Systems (ILS). By employing a continuous reference\\ntone, the propeller-induced Doppler spread can be effectively captured and\\nequalized, improving signal integrity and accuracy. Simulation results\\ndemonstrate that the proposed equalization method significantly reduces DDM\\ndeviation caused by propeller modulation, even under various propeller speeds.\\nThe findings suggest that incorporating such equalization techniques can\\nenhance the reliability and efficiency of drone-based RF inspections.\",\n          \"Mastering autonomous drone landing on dynamic platforms presents formidable\\nchallenges due to unpredictable velocities and external disturbances caused by\\nthe wind, ground effect, turbines or propellers of the docking platform. This\\nstudy introduces an advanced Deep Reinforcement Learning (DRL) agent,\\nLander:AI, designed to navigate and land on platforms in the presence of windy\\nconditions, thereby enhancing drone autonomy and safety. Lander:AI is\\nrigorously trained within the gym-pybullet-drone simulation, an environment\\nthat mirrors real-world complexities, including wind turbulence, to ensure the\\nagent's robustness and adaptability.\\n  The agent's capabilities were empirically validated with Crazyflie 2.1 drones\\nacross various test scenarios, encompassing both simulated environments and\\nreal-world conditions. The experimental results showcased Lander:AI's\\nhigh-precision landing and its ability to adapt to moving platforms, even under\\nwind-induced disturbances. Furthermore, the system performance was benchmarked\\nagainst a baseline PID controller augmented with an Extended Kalman Filter,\\nillustrating significant improvements in landing precision and error recovery.\\nLander:AI leverages bio-inspired learning to adapt to external forces like\\nbirds, enhancing drone adaptability without knowing force magnitudes.This\\nresearch not only advances drone landing technologies, essential for inspection\\nand emergency applications, but also highlights the potential of DRL in\\naddressing intricate aerodynamic challenges.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2308.09899v2\",\n          \"http://arxiv.org/pdf/2407.19084v1\",\n          \"http://arxiv.org/pdf/2403.06572v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c77a6527-9c7f-4bb2-9941-8f3cd7490c11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-06-29 01:16:01+00:00</td>\n",
              "      <td>EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following</td>\n",
              "      <td>The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\\nthreat to general security and confidentiality. Most of the commercially\\navailable or custom-built drones are multi-rotors and are comprised of multiple\\npropellers. Since these propellers rotate at a high-speed, they are generally\\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\\nclassical camera without severe motion blur. We utilize a class of sensors that\\nare particularly suitable for such scenarios called event cameras, which have a\\nhigh temporal resolution, low-latency, and high dynamic range.\\n  In this paper, we model the geometry of a propeller and use it to generate\\nsimulated events which are used to train a deep neural network called EVPropNet\\nto detect propellers from the data of an event camera. EVPropNet directly\\ntransfers to the real world without any fine-tuning or retraining. We present\\ntwo applications of our network: (a) tracking and following an unmarked drone\\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\\nthe proposed approach in many real-world experiments with different propeller\\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\\nbudget. To our knowledge, this is the first deep learning-based solution for\\ndetecting propellers (to detect drones). Finally, our applications also show an\\nimpressive success rate of 92% and 90% for the tracking and landing tasks\\nrespectively.</td>\n",
              "      <td>http://arxiv.org/pdf/2106.15045v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-07-26 21:05:04+00:00</td>\n",
              "      <td>Propeller Modulation Equalization via Reference Tones</td>\n",
              "      <td>Propeller modulation, also known as micro-Doppler modulation, presents a\\nsignificant challenge in radio frequency (RF) inspection operations conducted\\nvia drones. This paper investigates the equalization of propeller modulation\\neffects on RF signals, specifically targeting applications in navigation aids\\nsuch as Instrument Landing Systems (ILS). By employing a continuous reference\\ntone, the propeller-induced Doppler spread can be effectively captured and\\nequalized, improving signal integrity and accuracy. Simulation results\\ndemonstrate that the proposed equalization method significantly reduces DDM\\ndeviation caused by propeller modulation, even under various propeller speeds.\\nThe findings suggest that incorporating such equalization techniques can\\nenhance the reliability and efficiency of drone-based RF inspections.</td>\n",
              "      <td>http://arxiv.org/pdf/2407.19084v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-06-09 17:09:16+00:00</td>\n",
              "      <td>Autonomous Drone Landing with Fiducial Markers and a Gimbal-Mounted Camera for Active Tracking</td>\n",
              "      <td>Precision landing is a remaining challenge in autonomous drone flight.\\nFiducial markers provide a computationally cheap way for a drone to locate a\\nlanding pad and autonomously execute precision landings. However, most work in\\nthis field depends on either rigidly-mounted or downward-facing cameras which\\nrestrict the drone's ability to detect the marker. We present a method of\\nautonomous landing that uses a gimbal-mounted camera to quickly search for the\\nlanding pad by simply spinning in place while tilting the camera up and down,\\nand to continually aim the camera at the landing pad during approach and\\nlanding. This method demonstrates successful search, tracking, and landing with\\n4 of 5 tested fiducial systems on a physical drone with no human intervention.\\nPer fiducial system, we present the distributions of the distances from the\\ndrone to the center of the landing pad after each successful landing. We also\\nshow representative examples of flight trajectories, marker tracking\\nperformance, and control outputs for each channel during the landing. Finally,\\nwe discuss qualitative strengths and weaknesses underlying each system.</td>\n",
              "      <td>http://arxiv.org/pdf/2206.04617v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-02-15 04:04:24+00:00</td>\n",
              "      <td>Tombo Propeller: Bio-Inspired Deformable Structure toward Collision-Accommodated Control for Drones</td>\n",
              "      <td>There is a growing need for vertical take-off and landing vehicles, including\\ndrones, which are safe to use and can adapt to collisions. The risks of damage\\nby collision, to humans, obstacles in the environment, and drones themselves,\\nare significant. This has prompted a search into nature for a highly resilient\\nstructure that can inform a design of propellers to reduce those risks and\\nenhance safety. Inspired by the flexibility and resilience of dragonfly wings,\\nwe propose a novel design for a biomimetic drone propeller called Tombo\\npropeller. Here, we report on the design and fabrication process of this\\nbiomimetic propeller that can accommodate collisions and recover quickly, while\\nmaintaining sufficient thrust force to hover and fly. We describe the\\ndevelopment of an aerodynamic model and experiments conducted to investigate\\nperformance characteristics for various configurations of the propeller\\nmorphology, and related properties, such as generated thrust force, thrust\\nforce deviation, collision force, recovery time, lift-to-drag ratio, and noise.\\nFinally, we design and showcase a control strategy for a drone equipped with\\nTombo propellers that collides in mid-air with an obstacle and recovers from\\ncollision continuing flying. The results show that the maximum collision force\\ngenerated by the proposed Tombo propeller is less than two-thirds that of a\\ntraditional rigid propeller, which suggests the concrete possibility to employ\\ndeformable propellers for drones flying in a cluttered environment. This\\nresearch can contribute to morphological design of flying vehicles for agile\\nand resilient performance.</td>\n",
              "      <td>http://arxiv.org/pdf/2202.07177v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-11 06:13:44+00:00</td>\n",
              "      <td>Hundred Drones Land in a Minute</td>\n",
              "      <td>Currently, drone research and development has received significant attention\\nworldwide. Particularly, delivery services employ drones as it is a viable\\nmethod to improve delivery efficiency by using a several unmanned drones.\\nResearch has been conducted to realize complete automation of drone control for\\nsuch services. However, regarding the takeoff and landing port of the drones,\\nconventional methods have focused on the landing operation of a single drone,\\nand the continuous landing of multiple drones has not been realized. To address\\nthis issue, we propose a completely novel port system, \"EAGLES Port,\" that\\nallows several drones to continuously land and takeoff in a short time.\\nExperiments verified that the landing time efficiency of the proposed port is\\nideally 7.5 times higher than that of conventional vertical landing systems.\\nMoreover, the system can tolerate 270 mm of horizontal positional error, +-30\\ndeg of angular error in the drone's approach (+-40 deg with the proposed gate\\nmechanism), and up to 1.9 m/s of drone's approach speed. This technology\\nsignificantly contributes to the scalability of drone usage. Therefore, it is\\ncritical for the development of a future drone port for the landing of\\nautomated drone swarms.</td>\n",
              "      <td>http://arxiv.org/pdf/2003.05127v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-03-11 10:20:44+00:00</td>\n",
              "      <td>Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings</td>\n",
              "      <td>Mastering autonomous drone landing on dynamic platforms presents formidable\\nchallenges due to unpredictable velocities and external disturbances caused by\\nthe wind, ground effect, turbines or propellers of the docking platform. This\\nstudy introduces an advanced Deep Reinforcement Learning (DRL) agent,\\nLander:AI, designed to navigate and land on platforms in the presence of windy\\nconditions, thereby enhancing drone autonomy and safety. Lander:AI is\\nrigorously trained within the gym-pybullet-drone simulation, an environment\\nthat mirrors real-world complexities, including wind turbulence, to ensure the\\nagent's robustness and adaptability.\\n  The agent's capabilities were empirically validated with Crazyflie 2.1 drones\\nacross various test scenarios, encompassing both simulated environments and\\nreal-world conditions. The experimental results showcased Lander:AI's\\nhigh-precision landing and its ability to adapt to moving platforms, even under\\nwind-induced disturbances. Furthermore, the system performance was benchmarked\\nagainst a baseline PID controller augmented with an Extended Kalman Filter,\\nillustrating significant improvements in landing precision and error recovery.\\nLander:AI leverages bio-inspired learning to adapt to external forces like\\nbirds, enhancing drone adaptability without knowing force magnitudes.This\\nresearch not only advances drone landing technologies, essential for inspection\\nand emergency applications, but also highlights the potential of DRL in\\naddressing intricate aerodynamic challenges.</td>\n",
              "      <td>http://arxiv.org/pdf/2403.06572v2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-02-01 22:41:46+00:00</td>\n",
              "      <td>Autonomous Drone Landing: Marked Landing Pads and Solidified Lava Flows</td>\n",
              "      <td>Landing is the most challenging and risky aspect of multirotor drone flight,\\nand only simple landing methods exist for autonomous drones. We explore methods\\nfor autonomous drone landing in two scenarios. In the first scenario, we\\nexamine methods for landing on known landing pads using fiducial markers and a\\ngimbal-mounted monocular camera. This method has potential in drone\\napplications where a drone must land more accurately than GPS can provide\\n(e.g.~package delivery in an urban canyon). We expand on previous methods by\\nactuating the drone's camera to track the marker over time, and we address the\\ncomplexities of pose estimation caused by fiducial marker orientation\\nambiguity. In the second scenario, and in collaboration with the RAVEN project,\\nwe explore methods for landing on solidified lava flows in Iceland, which\\nserves as an analog environment for Mars and provides insight into the\\neffectiveness of drone-rover exploration teams. Our drone uses a depth camera\\nto visualize the terrain, and we are developing methods to analyze the terrain\\ndata for viable landing sites in real time with minimal sensors and external\\ninfrastructure requirements, so that the solution does not heavily influence\\nthe drone's behavior, mission structure, or operational environments.</td>\n",
              "      <td>http://arxiv.org/pdf/2302.00786v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-06-16 15:51:42+00:00</td>\n",
              "      <td>Autonomous Navigation System for a Delivery Drone</td>\n",
              "      <td>The use of delivery services is an increasing trend worldwide, further\\nenhanced by the COVID pandemic. In this context, drone delivery systems are of\\ngreat interest as they may allow for faster and cheaper deliveries. This paper\\npresents a navigation system that makes feasible the delivery of parcels with\\nautonomous drones. The system generates a path between a start and a final\\npoint and controls the drone to follow this path based on its localization\\nobtained through GPS, 9DoF IMU, and barometer. In the landing phase,\\ninformation of poses estimated by a marker (ArUco) detection technique using a\\ncamera, ultra-wideband (UWB) devices, and the drone's software estimation are\\nmerged by utilizing an Extended Kalman Filter algorithm to improve the landing\\nprecision. A vector field-based method controls the drone to follow the desired\\npath smoothly, reducing vibrations or harsh movements that could harm the\\ntransported parcel. Real experiments validate the delivery strategy and allow\\nto evaluate the performance of the adopted techniques. Preliminary results\\nstate the viability of our proposal for autonomous drone delivery.</td>\n",
              "      <td>http://arxiv.org/pdf/2106.08878v1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-08-19 03:57:52+00:00</td>\n",
              "      <td>Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models</td>\n",
              "      <td>Accurate drone detection is strongly desired in drone collision avoidance,\\ndrone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With\\nthe recent emergence of the Vision Transformer (ViT), this critical task is\\nreassessed in this paper using a UAV dataset composed of 1359 drone photos. We\\nconstruct various CNN and ViT-based models, demonstrating that for single-drone\\ndetection, a basic ViT can achieve performance 4.6 times more robust than our\\nbest CNN-based transfer learning models. By implementing the state-of-the-art\\nYou Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You\\nOnly Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we\\nattain impressive 98% and 96% mAP values, respectively. We find that ViT\\noutperforms CNN at the same epoch, but also requires more training data,\\ncomputational power, and sophisticated, performance-oriented designs to fully\\nsurpass the capabilities of cutting-edge CNN detectors. We summarize the\\ndistinct characteristics of ViT and CNN models to aid future researchers in\\ndeveloping more efficient deep learning models.</td>\n",
              "      <td>http://arxiv.org/pdf/2308.09899v2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-03-02 08:40:23+00:00</td>\n",
              "      <td>Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)</td>\n",
              "      <td>The imminent commercialization of UAM requires stable, AI-based maintenance\\nsystems to ensure safety for both passengers and pedestrians. This paper\\npresents a methodology for non-destructively detecting cracks in UAM propellers\\nusing drone propeller sound datasets. Normal operating sounds were recorded,\\nand abnormal sounds (categorized as ripped and broken) were differentiated by\\nvarying the microphone-propeller angle and throttle power. Our novel approach\\nintegrates FFT and STFT preprocessing techniques to capture both global\\nfrequency patterns and local time-frequency variations, thereby enhancing\\nanomaly detection performance. The constructed Acoustic Dataset for Crack of\\nDrone Propeller (ADCP) demonstrates the potential for detecting propeller\\ncracks and lays the groundwork for future UAM maintenance applications.</td>\n",
              "      <td>http://arxiv.org/pdf/2503.00790v1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c77a6527-9c7f-4bb2-9941-8f3cd7490c11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c77a6527-9c7f-4bb2-9941-8f3cd7490c11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c77a6527-9c7f-4bb2-9941-8f3cd7490c11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d8132411-9f09-4afc-ba9b-81f4b094a118\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8132411-9f09-4afc-ba9b-81f4b094a118')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d8132411-9f09-4afc-ba9b-81f4b094a118 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  published  \\\n",
              "0 2021-06-29 01:16:01+00:00   \n",
              "1 2024-07-26 21:05:04+00:00   \n",
              "2 2022-06-09 17:09:16+00:00   \n",
              "3 2022-02-15 04:04:24+00:00   \n",
              "4 2020-03-11 06:13:44+00:00   \n",
              "5 2024-03-11 10:20:44+00:00   \n",
              "6 2023-02-01 22:41:46+00:00   \n",
              "7 2021-06-16 15:51:42+00:00   \n",
              "8 2023-08-19 03:57:52+00:00   \n",
              "9 2025-03-02 08:40:23+00:00   \n",
              "\n",
              "                                                                                                                    title  \\\n",
              "0                                     EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following   \n",
              "1                                                                   Propeller Modulation Equalization via Reference Tones   \n",
              "2                          Autonomous Drone Landing with Fiducial Markers and a Gimbal-Mounted Camera for Active Tracking   \n",
              "3                     Tombo Propeller: Bio-Inspired Deformable Structure toward Collision-Accommodated Control for Drones   \n",
              "4                                                                                         Hundred Drones Land in a Minute   \n",
              "5                                Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings   \n",
              "6                                                 Autonomous Drone Landing: Marked Landing Pads and Solidified Lava Flows   \n",
              "7                                                                       Autonomous Navigation System for a Delivery Drone   \n",
              "8  Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models   \n",
              "9            Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                summary  \\\n",
              "0                                                                                         The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\\nthreat to general security and confidentiality. Most of the commercially\\navailable or custom-built drones are multi-rotors and are comprised of multiple\\npropellers. Since these propellers rotate at a high-speed, they are generally\\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\\nclassical camera without severe motion blur. We utilize a class of sensors that\\nare particularly suitable for such scenarios called event cameras, which have a\\nhigh temporal resolution, low-latency, and high dynamic range.\\n  In this paper, we model the geometry of a propeller and use it to generate\\nsimulated events which are used to train a deep neural network called EVPropNet\\nto detect propellers from the data of an event camera. EVPropNet directly\\ntransfers to the real world without any fine-tuning or retraining. We present\\ntwo applications of our network: (a) tracking and following an unmarked drone\\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\\nthe proposed approach in many real-world experiments with different propeller\\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\\nbudget. To our knowledge, this is the first deep learning-based solution for\\ndetecting propellers (to detect drones). Finally, our applications also show an\\nimpressive success rate of 92% and 90% for the tracking and landing tasks\\nrespectively.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Propeller modulation, also known as micro-Doppler modulation, presents a\\nsignificant challenge in radio frequency (RF) inspection operations conducted\\nvia drones. This paper investigates the equalization of propeller modulation\\neffects on RF signals, specifically targeting applications in navigation aids\\nsuch as Instrument Landing Systems (ILS). By employing a continuous reference\\ntone, the propeller-induced Doppler spread can be effectively captured and\\nequalized, improving signal integrity and accuracy. Simulation results\\ndemonstrate that the proposed equalization method significantly reduces DDM\\ndeviation caused by propeller modulation, even under various propeller speeds.\\nThe findings suggest that incorporating such equalization techniques can\\nenhance the reliability and efficiency of drone-based RF inspections.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Precision landing is a remaining challenge in autonomous drone flight.\\nFiducial markers provide a computationally cheap way for a drone to locate a\\nlanding pad and autonomously execute precision landings. However, most work in\\nthis field depends on either rigidly-mounted or downward-facing cameras which\\nrestrict the drone's ability to detect the marker. We present a method of\\nautonomous landing that uses a gimbal-mounted camera to quickly search for the\\nlanding pad by simply spinning in place while tilting the camera up and down,\\nand to continually aim the camera at the landing pad during approach and\\nlanding. This method demonstrates successful search, tracking, and landing with\\n4 of 5 tested fiducial systems on a physical drone with no human intervention.\\nPer fiducial system, we present the distributions of the distances from the\\ndrone to the center of the landing pad after each successful landing. We also\\nshow representative examples of flight trajectories, marker tracking\\nperformance, and control outputs for each channel during the landing. Finally,\\nwe discuss qualitative strengths and weaknesses underlying each system.   \n",
              "3  There is a growing need for vertical take-off and landing vehicles, including\\ndrones, which are safe to use and can adapt to collisions. The risks of damage\\nby collision, to humans, obstacles in the environment, and drones themselves,\\nare significant. This has prompted a search into nature for a highly resilient\\nstructure that can inform a design of propellers to reduce those risks and\\nenhance safety. Inspired by the flexibility and resilience of dragonfly wings,\\nwe propose a novel design for a biomimetic drone propeller called Tombo\\npropeller. Here, we report on the design and fabrication process of this\\nbiomimetic propeller that can accommodate collisions and recover quickly, while\\nmaintaining sufficient thrust force to hover and fly. We describe the\\ndevelopment of an aerodynamic model and experiments conducted to investigate\\nperformance characteristics for various configurations of the propeller\\nmorphology, and related properties, such as generated thrust force, thrust\\nforce deviation, collision force, recovery time, lift-to-drag ratio, and noise.\\nFinally, we design and showcase a control strategy for a drone equipped with\\nTombo propellers that collides in mid-air with an obstacle and recovers from\\ncollision continuing flying. The results show that the maximum collision force\\ngenerated by the proposed Tombo propeller is less than two-thirds that of a\\ntraditional rigid propeller, which suggests the concrete possibility to employ\\ndeformable propellers for drones flying in a cluttered environment. This\\nresearch can contribute to morphological design of flying vehicles for agile\\nand resilient performance.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                         Currently, drone research and development has received significant attention\\nworldwide. Particularly, delivery services employ drones as it is a viable\\nmethod to improve delivery efficiency by using a several unmanned drones.\\nResearch has been conducted to realize complete automation of drone control for\\nsuch services. However, regarding the takeoff and landing port of the drones,\\nconventional methods have focused on the landing operation of a single drone,\\nand the continuous landing of multiple drones has not been realized. To address\\nthis issue, we propose a completely novel port system, \"EAGLES Port,\" that\\nallows several drones to continuously land and takeoff in a short time.\\nExperiments verified that the landing time efficiency of the proposed port is\\nideally 7.5 times higher than that of conventional vertical landing systems.\\nMoreover, the system can tolerate 270 mm of horizontal positional error, +-30\\ndeg of angular error in the drone's approach (+-40 deg with the proposed gate\\nmechanism), and up to 1.9 m/s of drone's approach speed. This technology\\nsignificantly contributes to the scalability of drone usage. Therefore, it is\\ncritical for the development of a future drone port for the landing of\\nautomated drone swarms.   \n",
              "5                                                                                                         Mastering autonomous drone landing on dynamic platforms presents formidable\\nchallenges due to unpredictable velocities and external disturbances caused by\\nthe wind, ground effect, turbines or propellers of the docking platform. This\\nstudy introduces an advanced Deep Reinforcement Learning (DRL) agent,\\nLander:AI, designed to navigate and land on platforms in the presence of windy\\nconditions, thereby enhancing drone autonomy and safety. Lander:AI is\\nrigorously trained within the gym-pybullet-drone simulation, an environment\\nthat mirrors real-world complexities, including wind turbulence, to ensure the\\nagent's robustness and adaptability.\\n  The agent's capabilities were empirically validated with Crazyflie 2.1 drones\\nacross various test scenarios, encompassing both simulated environments and\\nreal-world conditions. The experimental results showcased Lander:AI's\\nhigh-precision landing and its ability to adapt to moving platforms, even under\\nwind-induced disturbances. Furthermore, the system performance was benchmarked\\nagainst a baseline PID controller augmented with an Extended Kalman Filter,\\nillustrating significant improvements in landing precision and error recovery.\\nLander:AI leverages bio-inspired learning to adapt to external forces like\\nbirds, enhancing drone adaptability without knowing force magnitudes.This\\nresearch not only advances drone landing technologies, essential for inspection\\nand emergency applications, but also highlights the potential of DRL in\\naddressing intricate aerodynamic challenges.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                    Landing is the most challenging and risky aspect of multirotor drone flight,\\nand only simple landing methods exist for autonomous drones. We explore methods\\nfor autonomous drone landing in two scenarios. In the first scenario, we\\nexamine methods for landing on known landing pads using fiducial markers and a\\ngimbal-mounted monocular camera. This method has potential in drone\\napplications where a drone must land more accurately than GPS can provide\\n(e.g.~package delivery in an urban canyon). We expand on previous methods by\\nactuating the drone's camera to track the marker over time, and we address the\\ncomplexities of pose estimation caused by fiducial marker orientation\\nambiguity. In the second scenario, and in collaboration with the RAVEN project,\\nwe explore methods for landing on solidified lava flows in Iceland, which\\nserves as an analog environment for Mars and provides insight into the\\neffectiveness of drone-rover exploration teams. Our drone uses a depth camera\\nto visualize the terrain, and we are developing methods to analyze the terrain\\ndata for viable landing sites in real time with minimal sensors and external\\ninfrastructure requirements, so that the solution does not heavily influence\\nthe drone's behavior, mission structure, or operational environments.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The use of delivery services is an increasing trend worldwide, further\\nenhanced by the COVID pandemic. In this context, drone delivery systems are of\\ngreat interest as they may allow for faster and cheaper deliveries. This paper\\npresents a navigation system that makes feasible the delivery of parcels with\\nautonomous drones. The system generates a path between a start and a final\\npoint and controls the drone to follow this path based on its localization\\nobtained through GPS, 9DoF IMU, and barometer. In the landing phase,\\ninformation of poses estimated by a marker (ArUco) detection technique using a\\ncamera, ultra-wideband (UWB) devices, and the drone's software estimation are\\nmerged by utilizing an Extended Kalman Filter algorithm to improve the landing\\nprecision. A vector field-based method controls the drone to follow the desired\\npath smoothly, reducing vibrations or harsh movements that could harm the\\ntransported parcel. Real experiments validate the delivery strategy and allow\\nto evaluate the performance of the adopted techniques. Preliminary results\\nstate the viability of our proposal for autonomous drone delivery.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Accurate drone detection is strongly desired in drone collision avoidance,\\ndrone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With\\nthe recent emergence of the Vision Transformer (ViT), this critical task is\\nreassessed in this paper using a UAV dataset composed of 1359 drone photos. We\\nconstruct various CNN and ViT-based models, demonstrating that for single-drone\\ndetection, a basic ViT can achieve performance 4.6 times more robust than our\\nbest CNN-based transfer learning models. By implementing the state-of-the-art\\nYou Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You\\nOnly Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we\\nattain impressive 98% and 96% mAP values, respectively. We find that ViT\\noutperforms CNN at the same epoch, but also requires more training data,\\ncomputational power, and sophisticated, performance-oriented designs to fully\\nsurpass the capabilities of cutting-edge CNN detectors. We summarize the\\ndistinct characteristics of ViT and CNN models to aid future researchers in\\ndeveloping more efficient deep learning models.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The imminent commercialization of UAM requires stable, AI-based maintenance\\nsystems to ensure safety for both passengers and pedestrians. This paper\\npresents a methodology for non-destructively detecting cracks in UAM propellers\\nusing drone propeller sound datasets. Normal operating sounds were recorded,\\nand abnormal sounds (categorized as ripped and broken) were differentiated by\\nvarying the microphone-propeller angle and throttle power. Our novel approach\\nintegrates FFT and STFT preprocessing techniques to capture both global\\nfrequency patterns and local time-frequency variations, thereby enhancing\\nanomaly detection performance. The constructed Acoustic Dataset for Crack of\\nDrone Propeller (ADCP) demonstrates the potential for detecting propeller\\ncracks and lays the groundwork for future UAM maintenance applications.   \n",
              "\n",
              "                                 url  \n",
              "0  http://arxiv.org/pdf/2106.15045v1  \n",
              "1  http://arxiv.org/pdf/2407.19084v1  \n",
              "2  http://arxiv.org/pdf/2206.04617v3  \n",
              "3  http://arxiv.org/pdf/2202.07177v1  \n",
              "4  http://arxiv.org/pdf/2003.05127v1  \n",
              "5  http://arxiv.org/pdf/2403.06572v2  \n",
              "6  http://arxiv.org/pdf/2302.00786v1  \n",
              "7  http://arxiv.org/pdf/2106.08878v1  \n",
              "8  http://arxiv.org/pdf/2308.09899v2  \n",
              "9  http://arxiv.org/pdf/2503.00790v1  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Found existing installation: transformers 4.52.4\n",
            "Uninstalling transformers-4.52.4:\n",
            "  Successfully uninstalled transformers-4.52.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "Successfully installed transformers-4.52.4\n",
            "Files removed: 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is a growing need for vertical take-off and landing vehicles, including drones, which are safe to use and can adapt to collisions. Inspired by the flexibility and resilience of dragonfly wings, we propose a novel design for a biomimetic drone propeller called Tombo. This research can contribute to morphological design of flying vehicles for agile and resilient performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarization_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyGIf-983k4v",
        "outputId": "c8c994f2-bee7-4582-9350-2210378d3776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'There is a growing need for vertical take-off and landing vehicles, including drones, which are safe to use and can adapt to collisions. Inspired by the flexibility and resilience of dragonfly wings, we propose a novel design for a biomimetic drone propeller called Tombo. This research can contribute to morphological design of flying vehicles for agile and resilient performance.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ],
      "metadata": {
        "id": "u4MrSaMfrSMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}